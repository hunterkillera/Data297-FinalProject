{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rpURTzAFukPi"},"outputs":[],"source":["from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","import re\n","import string\n","\n","# Preprocessing\n","def preprocess(sentence):\n","    # lowercase\n","    sentence = sentence.lower()\n","\n","    # remove punctuations\n","    sentence = \"\".join([char for char in sentence if char not in string.punctuation])\n","\n","    # remove numbers\n","    sentence = re.sub(r'\\d+', '', sentence)\n","\n","    words = sentence.split(\" \")\n","\n","    # remove stopwords\n","    stop_words = stopwords.words(\"english\")\n","    words = [word for word in words if word not in stop_words]\n","\n","    # lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","\n","\n","    return \" \".join(words)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"executionInfo":{"elapsed":341,"status":"error","timestamp":1702262332209,"user":{"displayName":"CTY M","userId":"07805765757895087110"},"user_tz":300},"id":"x7MIiEt2vJOd","outputId":"982bd9fe-8f61-4c15-9638-b2701a77b539"},"outputs":[],"source":["import pandas as pd\n","\n","def get_data(train_dir=train_dir, test_dir=test_dir, fn=fn, seeds=seeds, seed_idx=0):\n","    # load data\n","    train   = pd.read_excel(f\"{train_dir}/{fn}-train-{seeds[seed_idx]}.xlsx\")\n","    test    = pd.read_excel(f\"{test_dir}/{fn}-test-{seeds[seed_idx]}.xlsx\")\n","\n","    # preprocess the textual data\n","    train[\"sentence\"]   = train[\"sentence\"].apply(preprocess)\n","    test[\"sentence\"]    = test[\"sentence\"].apply(preprocess)\n","\n","    return train[\"sentence\"], test[\"sentence\"], train[\"label\"], test[\"label\"]"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702262371732,"user":{"displayName":"CTY M","userId":"07805765757895087110"},"user_tz":300},"id":"bt_bVhTXvkWC"},"outputs":[],"source":["import numpy as np\n","\n","def get_sentence_embedding_10k(sentence, embed, vocab_to_int):\n","\n","    vector_size = embed.shape[1]\n","    sentence_emd = np.zeros(vector_size)\n","    words = sentence.split()\n","\n","    for word in words:\n","        if word in vocab_to_int:\n","            # add the word vector if its embedding is present in w2v\n","            sentence_emd += embed[vocab_to_int[word]]\n","\n","    if len(words) > 0:\n","        # compute the element-wise average of the word embeddings\n","        sentence_emd = sentence_emd / len(words)\n","\n","    return sentence_emd"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":159,"status":"ok","timestamp":1702262388464,"user":{"displayName":"CTY M","userId":"07805765757895087110"},"user_tz":300},"id":"WXt642-XvVjV"},"outputs":[],"source":["import numpy as np\n","\n","def get_sentence_embedding_glove(sentence:str,glove_vectors)->np.ndarray:\n","\n","    vector_size = glove_vectors.vector_size\n","    sentence_emd = np.zeros(vector_size)\n","    words = sentence.split()\n","\n","    for word in words:\n","        if word in glove_vectors:\n","            # add the word vector if its embedding is present in w2v\n","            sentence_emd += glove_vectors[word]\n","\n","    if len(words) > 0:\n","        # compute the element-wise average of the word embeddings\n","        sentence_emd = sentence_emd / len(words)\n","\n","    return sentence_emd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mofgRg_nvrPw"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNIP8DihRGicz1frz91rp7J","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
