{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import time, sleep\n",
    "import pandas as pd\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, RobertaTokenizerFast, RobertaForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification, XLNetForSequenceClassification, XLNetTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lab-manual-combine-training data\n",
    "\n",
    "# Read an Excel file using pandas\n",
    "mm_5768 = '/Users/simonli/Desktop/data297/fomc-hawkish-dovish-main/training_data/test-and-training/training_data/lab-manual-mm-split-train-5768.xlsx'\n",
    "df_5768 = pd.read_excel(mm_5768)\n",
    "\n",
    "mm_78516 = '/Users/simonli/Desktop/data297/fomc-hawkish-dovish-main/training_data/test-and-training/training_data/lab-manual-mm-split-train-78516.xlsx'\n",
    "df_78516 = pd.read_excel(mm_78516)\n",
    "\n",
    "mm_944601 = '/Users/simonli/Desktop/data297/fomc-hawkish-dovish-main/training_data/test-and-training/training_data/lab-manual-mm-split-train-944601.xlsx'\n",
    "df_944601 = pd.read_excel(mm_944601)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an Excel file using pandas\n",
    "mm_5768_test = '/Users/simonli/Desktop/data297/fomc-hawkish-dovish-main/training_data/test-and-training/test_data/lab-manual-combine-test-5768.xlsx'\n",
    "df_5768_test = pd.read_excel(mm_5768_test)\n",
    "\n",
    "\n",
    "mm_78516_test = '/Users/simonli/Desktop/data297/fomc-hawkish-dovish-main/training_data/test-and-training/test_data/lab-manual-combine-test-78516.xlsx'\n",
    "df_78516_test = pd.read_excel(mm_78516_test)\n",
    "\n",
    "mm_944601_test = '/Users/simonli/Desktop/data297/fomc-hawkish-dovish-main/training_data/test-and-training/test_data/lab-manual-combine-test-944601.xlsx'\n",
    "df_944601_test = pd.read_excel(mm_944601_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm_hawkish_dovish(gpu_numbers: str,train_data_path: str, test_data_path: str, language_model_to_use: str, seed: int, batch_size: int, learning_rate: float, save_model_path: str):\n",
    "    \"\"\"\n",
    "    Description: Run experiment over particular batch size, learning rate and seed\n",
    "    \"\"\"\n",
    "     # set gpu\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_numbers)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(\"Device assigned: \", device)\n",
    "\n",
    "    # load training data\n",
    "    data_df = pd.read_excel(train_data_path)\n",
    "    sentences = data_df['sentence'].to_list()\n",
    "    labels = data_df['label'].to_numpy()\n",
    "\n",
    "    # load test data\n",
    "    data_df_test = pd.read_excel(test_data_path)\n",
    "    sentences_test = data_df_test['sentence'].to_list()\n",
    "    labels_test = data_df_test['label'].to_numpy()\n",
    "\n",
    "    # load tokenizer\n",
    "    try:\n",
    "        if language_model_to_use == 'bert':\n",
    "            tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'roberta':\n",
    "            tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'flangroberta':\n",
    "            tokenizer = AutoTokenizer.from_pretrained('SALT-NLP/FLANG-Roberta', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'finbert':\n",
    "            tokenizer = BertTokenizerFast(vocab_file='../finbert-uncased/FinVocab-Uncased.txt', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'flangbert':\n",
    "            tokenizer = BertTokenizerFast.from_pretrained('SALT-NLP/FLANG-BERT', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'bert-large':\n",
    "            tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'roberta-large':\n",
    "            tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'pretrain_roberta':\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"../pretrained_roberta_output\", do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'xlnet':\n",
    "            tokenizer = XLNetTokenizerFast.from_pretrained(\"xlnet-base-cased\", do_lower_case=True, do_basic_tokenize=True)\n",
    "        else:\n",
    "            return -1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sleep(600)\n",
    "        if language_model_to_use == 'bert':\n",
    "            tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'roberta':\n",
    "            tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'flangroberta':\n",
    "            tokenizer = AutoTokenizer.from_pretrained('SALT-NLP/FLANG-Roberta', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'finbert':\n",
    "            tokenizer = BertTokenizerFast(vocab_file='../finbert-uncased/FinVocab-Uncased.txt', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'flangbert':\n",
    "            tokenizer = BertTokenizerFast.from_pretrained('SALT-NLP/FLANG-BERT', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'bert-large':\n",
    "            tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'roberta-large':\n",
    "            tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large', do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'pretrain_roberta':\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"../pretrained_roberta_output\", do_lower_case=True, do_basic_tokenize=True)\n",
    "        elif language_model_to_use == 'xlnet':\n",
    "            tokenizer = XLNetTokenizerFast.from_pretrained(\"xlnet-base-cased\", do_lower_case=True, do_basic_tokenize=True)\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    max_length = 0\n",
    "    sentence_input = []\n",
    "    labels_output = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = tokenizer(sentence)['input_ids']\n",
    "            sentence_input.append(sentence)\n",
    "            max_length = max(max_length, len(tokens))\n",
    "            labels_output.append(labels[i])\n",
    "        else:\n",
    "            pass\n",
    "    max_length=256\n",
    "    if language_model_to_use == 'flangroberta':\n",
    "        max_length=128\n",
    "    tokens = tokenizer(sentence_input, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "    labels = np.array(labels_output)\n",
    "\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_masks = tokens['attention_mask']\n",
    "    labels = torch.LongTensor(labels)\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    val_length = int(len(dataset) * 0.2)\n",
    "    train_length = len(dataset) - val_length\n",
    "    print(f'Train Size: {train_length}, Validation Size: {val_length}')\n",
    "    experiment_results = []\n",
    "    \n",
    "    # assign seed to numpy and PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed) \n",
    "\n",
    "    # select language model\n",
    "    try: \n",
    "        if language_model_to_use == 'bert':\n",
    "            model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'roberta':\n",
    "            model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'flangroberta':\n",
    "            model = AutoModelForSequenceClassification.from_pretrained('SALT-NLP/FLANG-Roberta', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'finbert':\n",
    "            model = BertForSequenceClassification.from_pretrained('../finbert-uncased/model', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'flangbert':\n",
    "            model = BertForSequenceClassification.from_pretrained('SALT-NLP/FLANG-BERT', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'bert-large':\n",
    "            model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'roberta-large':\n",
    "            model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'pretrain_roberta':\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\"../pretrained_roberta_output\", num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'xlnet':\n",
    "            model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=3).to(device)\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        sleep(600)\n",
    "        if language_model_to_use == 'bert':\n",
    "            model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'roberta':\n",
    "            model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'flangroberta':\n",
    "            model = AutoModelForSequenceClassification.from_pretrained('SALT-NLP/FLANG-Roberta', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'finbert':\n",
    "            model = BertForSequenceClassification.from_pretrained('../finbert-uncased/model', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'flangbert':\n",
    "            model = BertForSequenceClassification.from_pretrained('SALT-NLP/FLANG-BERT', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'bert-large':\n",
    "            model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'roberta-large':\n",
    "            model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'pretrain_roberta':\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\"../pretrained_roberta_output\", num_labels=3).to(device)\n",
    "        elif language_model_to_use == 'xlnet':\n",
    "            model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=3).to(device)\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    # create train-val split\n",
    "    train, val = torch.utils.data.random_split(dataset=dataset, lengths=[train_length, val_length])\n",
    "    dataloaders_dict = {'train': DataLoader(train, batch_size=batch_size, shuffle=True), 'val': DataLoader(val, batch_size=batch_size, shuffle=True)}\n",
    "    print(train_length, val_length)\n",
    "    # select optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    max_num_epochs = 100\n",
    "    max_early_stopping = 7\n",
    "    early_stopping_count = 0\n",
    "    best_ce = float('inf')\n",
    "    best_accuracy = float('-inf')\n",
    "    best_f1 = float('-inf')\n",
    "\n",
    "    eps = 1e-2\n",
    "\n",
    "    for epoch in range(max_num_epochs):\n",
    "        if (early_stopping_count >= max_early_stopping):\n",
    "            break\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                early_stopping_count += 1\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            curr_ce = 0\n",
    "            curr_accuracy = 0\n",
    "            actual = torch.tensor([]).long().to(device)\n",
    "            pred = torch.tensor([]).long().to(device)\n",
    "\n",
    "            for input_ids, attention_masks, labels in dataloaders_dict[phase]:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_masks = attention_masks.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(input_ids = input_ids, attention_mask = attention_masks, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    else:\n",
    "                        curr_ce += loss.item() * input_ids.size(0)\n",
    "                        curr_accuracy += torch.sum(torch.max(outputs.logits, 1)[1] == labels).item()\n",
    "                        actual = torch.cat([actual, labels], dim=0)\n",
    "                        pred= torch.cat([pred, torch.max(outputs.logits, 1)[1]], dim=0)\n",
    "            if phase == 'val':\n",
    "                curr_ce = curr_ce / len(val)\n",
    "                curr_accuracy = curr_accuracy / len(val)\n",
    "                currF1 = f1_score(actual.cpu().detach().numpy(), pred.cpu().detach().numpy(), average='weighted')\n",
    "                if curr_ce <= best_ce - eps:\n",
    "                    best_ce = curr_ce\n",
    "                    early_stopping_count = 0\n",
    "                if curr_accuracy >= best_accuracy + eps:\n",
    "                    best_accuracy = curr_accuracy\n",
    "                    early_stopping_count = 0\n",
    "                if currF1 >= best_f1 + eps:\n",
    "                    best_f1 = currF1\n",
    "                    early_stopping_count = 0\n",
    "                print(\"Val CE: \", curr_ce)\n",
    "                print(\"Val Accuracy: \", curr_accuracy)\n",
    "                print(\"Val F1: \", currF1)\n",
    "                print(\"Early Stopping Count: \", early_stopping_count)\n",
    "    \n",
    "    ## ------------------testing---------------------\n",
    "    sentence_input_test = []\n",
    "    labels_output_test = []\n",
    "    for i, sentence in enumerate(sentences_test):\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = tokenizer(sentence)['input_ids']\n",
    "            sentence_input_test.append(sentence)\n",
    "            labels_output_test.append(labels_test[i])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    tokens_test = tokenizer(sentence_input_test, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "    labels_test = np.array(labels_output_test)\n",
    "\n",
    "    input_ids_test = tokens_test['input_ids']\n",
    "    attention_masks_test = tokens_test['attention_mask']\n",
    "    labels_test = torch.LongTensor(labels_test)\n",
    "    dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "    dataloaders_dict_test = {'test': DataLoader(dataset_test, batch_size=batch_size, shuffle=True)}\n",
    "    test_ce = 0\n",
    "    test_accuracy = 0\n",
    "    actual = torch.tensor([]).long().to(device)\n",
    "    pred = torch.tensor([]).long().to(device)\n",
    "    for input_ids, attention_masks, labels in dataloaders_dict_test['test']:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)   \n",
    "        optimizer.zero_grad()   \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            test_ce += loss.item() * input_ids.size(0)\n",
    "            test_accuracy += torch.sum(torch.max(outputs.logits, 1)[1] == labels).item()\n",
    "            actual = torch.cat([actual, labels], dim=0)\n",
    "            pred = torch.cat([pred, torch.max(outputs.logits, 1)[1]], dim=0)\n",
    "    test_ce = test_ce / len(dataset_test)\n",
    "    test_accuracy = test_accuracy/ len(dataset_test)\n",
    "    test_f1 = f1_score(actual.cpu().detach().numpy(), pred.cpu().detach().numpy(), average='weighted')\n",
    "    experiment_results = [seed, learning_rate, batch_size, best_ce, best_accuracy, best_f1, test_ce, test_accuracy, test_f1]\n",
    "\n",
    "    # save model\n",
    "    if save_model_path != None:\n",
    "        model.save_pretrained(save_model_path)\n",
    "        tokenizer.save_pretrained(save_model_path)\n",
    "\n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device assigned:  cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddf770edfa64ee385c534eceb0083ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfa6f1393b542b1aa85024e2baed244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6e025cc3be4e6e94167a26c0337a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d266f1390b724c5e8b9fc33fac86a160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 724, Validation Size: 181\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6516816ee7b5444aa98c6b3c5d6271e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724 181\n",
      "Val CE:  1.1116021028539753\n",
      "Val Accuracy:  0.3756906077348066\n",
      "Val F1:  0.20519647651378997\n",
      "Early Stopping Count:  0\n",
      "Val CE:  1.0829689963746465\n",
      "Val Accuracy:  0.3701657458563536\n",
      "Val F1:  0.26684411259014945\n",
      "Early Stopping Count:  0\n",
      "Val CE:  1.1453842138717187\n",
      "Val Accuracy:  0.39226519337016574\n",
      "Val F1:  0.34787262531250146\n",
      "Early Stopping Count:  0\n",
      "Val CE:  1.0832097095679183\n",
      "Val Accuracy:  0.4861878453038674\n",
      "Val F1:  0.47886585018826827\n",
      "Early Stopping Count:  0\n",
      "Val CE:  1.1075177633959945\n",
      "Val Accuracy:  0.49171270718232046\n",
      "Val F1:  0.4875687900163429\n",
      "Early Stopping Count:  1\n",
      "Val CE:  1.0486321172661544\n",
      "Val Accuracy:  0.6077348066298343\n",
      "Val F1:  0.6106835726353647\n",
      "Early Stopping Count:  0\n",
      "Val CE:  1.1707473775958488\n",
      "Val Accuracy:  0.6022099447513812\n",
      "Val F1:  0.5990457592214004\n",
      "Early Stopping Count:  1\n",
      "Val CE:  1.2982947217166754\n",
      "Val Accuracy:  0.5911602209944752\n",
      "Val F1:  0.593791945279117\n",
      "Early Stopping Count:  2\n",
      "Val CE:  1.3955862568228283\n",
      "Val Accuracy:  0.569060773480663\n",
      "Val F1:  0.573118884933021\n",
      "Early Stopping Count:  3\n",
      "Val CE:  1.5035118878875648\n",
      "Val Accuracy:  0.580110497237569\n",
      "Val F1:  0.5801325000939273\n",
      "Early Stopping Count:  4\n",
      "Val CE:  1.5187778291781304\n",
      "Val Accuracy:  0.580110497237569\n",
      "Val F1:  0.5820498946945315\n",
      "Early Stopping Count:  5\n",
      "Val CE:  1.6848178548707489\n",
      "Val Accuracy:  0.569060773480663\n",
      "Val F1:  0.5724950015059952\n",
      "Early Stopping Count:  6\n",
      "Val CE:  1.7046701618321034\n",
      "Val Accuracy:  0.6132596685082873\n",
      "Val F1:  0.6141412272756374\n",
      "Early Stopping Count:  7\n"
     ]
    }
   ],
   "source": [
    "#call train_lm_hawkish_dovish\n",
    "train_data = '/Users/simonli/Desktop/data297/fomc-hawkish-dovish-main/training_data/test-and-training/training_data/lab-manual-mm-split-train-5768.xlsx'\n",
    "test_data = '/Users/simonli/Desktop/data297/fomc-hawkish-dovish-main/training_data/test-and-training/test_data/lab-manual-mm-split-test-5768.xlsx'\n",
    "result = train_lm_hawkish_dovish(gpu_numbers=0,train_data_path=train_data, test_data_path=test_data, language_model_to_use='bert', seed=1, batch_size=16, learning_rate=1e-5, save_model_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1e-05,\n",
       " 16,\n",
       " 1.0486321172661544,\n",
       " 0.6077348066298343,\n",
       " 0.6106835726353647,\n",
       " 1.4094074013999904,\n",
       " 0.6343612334801763,\n",
       " 0.6338137193203273]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml135_env_sp23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
